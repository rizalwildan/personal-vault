# Story 3.2: Transformers.js Embedding Service

**Status:** Draft

**Epic:** Epic 3 - Backend Notes CRUD & Embedding Generation

---

## Story

**As a** backend developer,
**I want** to integrate Transformers.js for multilingual embedding generation with async job queue,
**so that** notes can be converted to vector embeddings asynchronously without blocking API responses.

---

## Acceptance Criteria

- [ ] `@xenova/transformers` package installed and configured in backend
- [ ] Embedding service loads `paraphrase-multilingual-MiniLM-L12-v2` model on first use
- [ ] Model is cached in `.cache/transformers/` directory (Transformers.js default behavior)
- [ ] `generateEmbedding(text)` function returns Float32Array with exactly 384 dimensions
- [ ] Embedding generation completes in 300-500ms per note (measured, acceptable)
- [ ] Text preprocessing removes markdown formatting (#\*\_~` symbols, link syntax)
- [ ] Text truncated to 2000 characters (approximately 512 tokens)
- [ ] Async job queue manages embedding generation without blocking user requests
- [ ] Queue processes up to 5 concurrent embeddings (MAX_CONCURRENT = 5)
- [ ] Embedding status progresses: 'pending' → 'processing' → 'completed' (or 'failed')
- [ ] Retry logic with exponential backoff: 1s, 2s, 4s (max 3 retries)
- [ ] After 3 retries, embedding status marked as 'failed' permanently
- [ ] Duplicate processing prevented (same noteId cannot be processed twice concurrently)
- [ ] Error logging for failed embeddings with retry count
- [ ] EmbeddingService singleton initialized at application startup
- [ ] Health check endpoint `/health` includes embedding service status (model, dimensions, queue size)
- [ ] Unit tests for text preprocessing (markdown removal, truncation)
- [ ] Unit tests for embedding generation (dimension validation, error handling)
- [ ] Integration tests for embedding queue (concurrent processing, retries, duplicate prevention)
- [ ] Integration tests for embedding service status endpoint

---

## Tasks / Subtasks

- [ ] Task 1: Install and Configure Dependencies (AC: 1)
  - [ ] Add `@xenova/transformers` to `backend/package.json`
  - [ ] Add `@types/node` for TypeScript support (if not already present)
  - [ ] Run `pnpm install` or `pnpm add` to install packages
  - [ ] Verify `@xenova/transformers` version matches tech stack

- [ ] Task 2: Create Embedding Service (AC: 2, 3, 4, 5, 6, 7)
  - [ ] Create `backend/src/services/embedding.service.ts` file
  - [ ] Import and configure Transformers.js: `pipeline` from `@xenova/transformers`
  - [ ] Set cache directory: `transformersEnv.cacheDir = './.cache/transformers'`
  - [ ] Define EmbeddingService class with private fields: model, isInitialized, MODEL_NAME, DIMENSIONS, processingQueue
  - [ ] Implement `initialize()` method to load model asynchronously on first use
  - [ ] Implement `generateEmbedding(text)` async method that returns Float32Array
  - [ ] Validate embedding dimensions (must be 384)
  - [ ] Create singleton export: `export const embeddingService = new EmbeddingService()`
  - [ ] Test model initialization with console.log statements for debugging

- [ ] Task 3: Implement Text Preprocessing (AC: 6, 7)
  - [ ] Create `preprocessText(text)` function within embedding service
  - [ ] Remove markdown formatting: `replace(/[#*_~`]/g, '')` for symbols
  - [ ] Extract link text: `replace(/\[([^\]]+)\]\([^\)]+\)/g, '$1')`
  - [ ] Replace newlines: `replace(/\n+/g, ' ')` to remove whitespace
  - [ ] Apply `.trim()` after cleaning
  - [ ] Truncate to 2000 characters (rough ~512 tokens estimate)
  - [ ] Return cleaned text to embedding generation

- [ ] Task 4: Create Embedding Job Queue (AC: 8, 9, 10)
  - [ ] Create `backend/src/queues/embedding.queue.ts` file
  - [ ] Define EmbeddingQueue class with fields: queue (string array), isProcessing, MAX_CONCURRENT (=5), processing (Set<string>)
  - [ ] Implement `enqueue(noteId)` async method to add noteId to queue
  - [ ] Call `process()` from `enqueue()` to trigger non-blocking processing
  - [ ] Implement private `process()` async method that manages concurrent processing
  - [ ] Process loop: while queue has items or items are processing
  - [ ] Concurrent limit: process only MAX_CONCURRENT items simultaneously
  - [ ] Dequeue items and process via `embeddingService.processNote(noteId)`
  - [ ] Set small delay (100ms) between checks to prevent CPU spinning
  - [ ] Implement `getStatus()` method returning queue size and processing count
  - [ ] Create singleton export: `export const embeddingQueue = new EmbeddingQueue()`

- [ ] Task 5: Implement processNote with Retry Logic (AC: 10, 11, 12, 13)
  - [ ] Implement `processNote(noteId, retryCount = 0)` method in EmbeddingService
  - [ ] Check processingQueue to prevent duplicate processing
  - [ ] Add noteId to processingQueue
  - [ ] Update embedding status to 'processing' in database
  - [ ] Fetch note content from notesRepository
  - [ ] Call `generateEmbedding()` with note content
  - [ ] Store embedding in database with status 'completed'
  - [ ] Log success: `✅ Embedding generated for note ${noteId}`
  - [ ] Implement error handling in try/catch/finally
  - [ ] On error: check retryCount < 3
  - [ ] Apply exponential backoff: `const delay = Math.pow(2, retryCount) * 1000;`
  - [ ] Wait then recursively call `processNote(noteId, retryCount + 1)`
  - [ ] After 3 retries: set embedding status to 'failed'
  - [ ] Log error: `❌ Embedding permanently failed for note ${noteId} after 3 retries`
  - [ ] Always delete noteId from processingQueue in finally block
  - [ ] Handle case where note not found in database

- [ ] Task 6: Initialize Embedding Service at Startup (AC: 15)
  - [ ] Modify `backend/src/index.ts` (application entry point)
  - [ ] Add `embeddingService.initialize()` call during bootstrap
  - [ ] Wrap in try/catch to handle init failures
  - [ ] Log success: `✅ Embedding model loaded`
  - [ ] Log error and exit if initialization fails: `process.exit(1)`
  - [ ] Call after database verification, before server.listen()

- [ ] Task 7: Create Health Check Endpoint for Embedding Status (AC: 16)
  - [ ] Create or update `backend/src/routes/health.routes.ts`
  - [ ] Add endpoint: `GET /health` or `GET /api/v1/health`
  - [ ] Return JSON with embedding service status: `embeddingService.getStatus()`
  - [ ] Include: isInitialized, model name, dimensions, queueSize
  - [ ] Return HTTP 200 with status information
  - [ ] Example response:
    ```json
    {
      "status": "healthy",
      "embedding": {
        "isInitialized": true,
        "model": "Xenova/paraphrase-multilingual-MiniLM-L12-v2",
        "dimensions": 384,
        "queueSize": 2
      }
    }
    ```

- [ ] Task 8: Unit Tests for Text Preprocessing (AC: 17)
  - [ ] Create `backend/tests/unit/services/text-preprocessing.test.ts`
  - [ ] Test 1: Markdown symbols removed correctly
    - [ ] Input: `# Title with *bold* and _italic_`
    - [ ] Expected: `Title with bold and italic`
  - [ ] Test 2: Link syntax extracted (text preserved)
    - [ ] Input: `See [this guide](https://example.com) for more`
    - [ ] Expected: `See this guide for more`
  - [ ] Test 3: Newlines collapsed to spaces
    - [ ] Input: `Line 1\n\nLine 2`
    - [ ] Expected: `Line 1 Line 2`
  - [ ] Test 4: Text truncated to 2000 characters
    - [ ] Input: String of 3000+ characters
    - [ ] Expected: Exactly 2000 characters or less
  - [ ] Test 5: Whitespace trimmed
    - [ ] Input: `  leading and trailing spaces  `
    - [ ] Expected: `leading and trailing spaces`
  - [ ] Test 6: Complex markdown removed (code blocks, tables)
    - [ ] Input: Markdown with `code` and | table cells |
    - [ ] Expected: Cleaned text without special characters

- [ ] Task 9: Unit Tests for Embedding Generation (AC: 18)
  - [ ] Create `backend/tests/unit/services/embedding.service.test.ts`
  - [ ] Test 1: Model initialization successful
    - [ ] Call `embeddingService.initialize()`
    - [ ] Verify `isInitialized` flag is true
    - [ ] Verify model loaded (no error thrown)
  - [ ] Test 2: generateEmbedding returns 384-dimensional Float32Array
    - [ ] Call `generateEmbedding('test text')`
    - [ ] Assert result is Float32Array
    - [ ] Assert result.length === 384
  - [ ] Test 3: Embedding generation completes within timeout (300-500ms)
    - [ ] Measure execution time
    - [ ] Assert time < 1000ms (generous timeout for CI)
  - [ ] Test 4: Service throws error if not initialized
    - [ ] Don't call initialize()
    - [ ] Call generateEmbedding()
    - [ ] Assert throws with message 'Embedding service not initialized'
  - [ ] Test 5: Different text inputs produce different embeddings
    - [ ] Generate embeddings for 'hello world' and 'goodbye world'
    - [ ] Assert embeddings are different (not identical arrays)
  - [ ] Test 6: Same text generates consistent embedding (deterministic)
    - [ ] Generate embedding twice for same text
    - [ ] Assert both embeddings are identical

- [ ] Task 10: Integration Tests for Embedding Queue (AC: 19, 9, 10, 11, 12, 13)
  - [ ] Create `backend/tests/integration/queues/embedding.queue.test.ts`
  - [ ] Setup: Create test user and note in database
  - [ ] Teardown: Clean up test data after each test
  - [ ] Test 1: Enqueue triggers background processing
    - [ ] Create test note with status 'pending'
    - [ ] Call `embeddingQueue.enqueue(noteId)`
    - [ ] Wait for processing (poll queue status or use Promise)
    - [ ] Verify note embedding_status updated to 'completed'
    - [ ] Verify embedding stored in database (not null)
  - [ ] Test 2: Multiple concurrent notes processed (up to MAX_CONCURRENT=5)
    - [ ] Create 5 test notes
    - [ ] Enqueue all 5 simultaneously
    - [ ] Verify all 5 complete embeddings within timeout
    - [ ] Verify no errors in processing
  - [ ] Test 3: Queue processes correctly when > MAX_CONCURRENT items queued
    - [ ] Create 10 test notes
    - [ ] Enqueue all 10
    - [ ] Verify first 5 processed
    - [ ] Verify remaining 5 processed after first batch completes
    - [ ] Verify all 10 have completed embeddings
  - [ ] Test 4: Duplicate processing prevented
    - [ ] Create test note
    - [ ] Call `embeddingQueue.enqueue(noteId)` twice in quick succession
    - [ ] Verify processing occurs only once (no duplicate embeddings)
    - [ ] Verify processingQueue doesn't contain duplicates
  - [ ] Test 5: Retry logic works (simulate temporary failure)
    - [ ] Mock embeddingService to fail 2 times then succeed
    - [ ] Enqueue note
    - [ ] Verify embedding eventually succeeds after retries
    - [ ] Verify status progression: pending → processing → completed
  - [ ] Test 6: Permanent failure after 3 retries
    - [ ] Mock embeddingService to always fail
    - [ ] Enqueue note
    - [ ] Wait for processing to complete
    - [ ] Verify embedding_status is 'failed'
    - [ ] Verify no further retries attempted

- [ ] Task 11: Integration Tests for Health Check Endpoint (AC: 16)
  - [ ] Create `backend/tests/integration/routes/health.routes.test.ts`
  - [ ] Test 1: GET /health returns embedding service status
    - [ ] Make GET request to /health
    - [ ] Assert HTTP 200 response
    - [ ] Assert response contains embedding.isInitialized = true
    - [ ] Assert response contains embedding.model = 'Xenova/paraphrase-multilingual-MiniLM-L12-v2'
    - [ ] Assert response contains embedding.dimensions = 384
    - [ ] Assert response contains embedding.queueSize >= 0
  - [ ] Test 2: Queue size updates in health endpoint
    - [ ] Create note and enqueue embedding
    - [ ] Make GET /health request
    - [ ] Assert queueSize increases
    - [ ] Wait for processing
    - [ ] Make second GET /health request
    - [ ] Assert queueSize decreases

---

## Dev Notes

### Previous Story Context

Story 3.1 (Database Schema) completed successfully. Notes table now has:

- `embedding` VECTOR(384) column
- `embedding_status` enum field with values: 'pending', 'processing', 'completed', 'failed'
- Triggers to auto-reset embedding on content change
- HNSW indexes optimized for pgvector operations

This story builds on that schema to populate embeddings.

### Technology Stack

**ML Inference Framework:** @xenova/transformers

- [Source: architecture/tech-stack.md]
- Package: `@xenova/transformers` (Hugging Face Transformers.js)
- Model: `paraphrase-multilingual-MiniLM-L12-v2` (384-dim, 50+ languages)
- Performance: 300-500ms per embedding (CPU inference via ONNX)
- Memory: ~120MB for model in-memory cache

**Backend Runtime/ORM (already configured in Story 3.1):**

- Runtime: Bun 1.x
- Framework: Elysia.js
- Language: TypeScript 5.7
- ORM: Drizzle ORM (for database access)

### Embedding Service Architecture

[Source: architecture/backend-architecture.md#Service-Layer-Embedding-Service]

**Core Responsibilities:**

1. Load ML model on first use (lazy initialization)
2. Generate 384-dimensional embeddings from text
3. Manage processing queue to prevent duplicate work
4. Track embedding status lifecycle

**Key Implementation Details:**

- Model loading is async and happens once (cached in memory)
- Cache directory: `.cache/transformers/` - Transformers.js automatically manages this
- Model name must be exact: `Xenova/paraphrase-multilingual-MiniLM-L12-v2`
- Pipeline type: 'feature-extraction' for embeddings
- Pooling: 'mean' (average all token embeddings)
- Normalize: true (unit vectors for cosine similarity)
- Output: Float32Array directly from `model()` call

**Singleton Pattern:**

```typescript
class EmbeddingService {
  private static instance: EmbeddingService;
  // ... implementation
}
export const embeddingService = new EmbeddingService();
```

### Async Job Queue Architecture

[Source: architecture/backend-architecture.md#Background-Job-Queue]

**Queue Purpose:**

- Non-blocking embedding generation
- Prevents model hogging when multiple embeddings needed
- Manages concurrent processing limit

**Queue Characteristics:**

- In-memory queue (simple array for MVP, no Redis)
- Max concurrent: 5 embeddings processed simultaneously
- FIFO processing order
- Automatic deduplication (prevent same noteId processed twice)

**Queue Flow:**

1. API creates note → calls `embeddingQueue.enqueue(noteId)`
2. Queue adds noteId to internal queue
3. Process loop checks for available capacity
4. Dequeues noteId and calls `embeddingService.processNote(noteId)`
5. Processing happens in background (non-blocking)
6. On completion/error, noteId removed from processing set
7. Process loop continues with next item

**Retry Strategy:**

- Exponential backoff: 1s → 2s → 4s delays between retries
- Max 3 retry attempts total
- After 3 retries: mark as 'failed' permanently
- Log all retry attempts for debugging

### Database Integration

[Source: shared/schemas/note.ts - from Story 3.1]

**Database Operations Needed:**

- `notesRepository.findById(noteId)` - Get note content
- `notesRepository.updateEmbeddingStatus(noteId, status)` - Update status field
- `notesRepository.updateEmbedding(noteId, embedding, status)` - Store vector + status

**Embedding Status Lifecycle:**

- 'pending': Initial state when note created (or content changed)
- 'processing': Actively generating embedding
- 'completed': Successfully stored embedding (384-dim vector)
- 'failed': Permanent failure after 3 retries

**Important:** Status field is VARCHAR(20) with CHECK constraint, so must use exact lowercase values.

### Application Startup Sequence

[Source: architecture/backend-architecture.md#Application-Entry-Point]

**Startup Order (in `backend/src/index.ts`):**

1. Verify database connection (await db.execute('SELECT 1'))
2. Initialize embedding service (await embeddingService.initialize())
3. Start HTTP server (app.listen(PORT))

**Rationale:**

- Database must be ready before initializing service
- Embedding model must be loaded before processing requests
- Model loading (~500MB, 2-3 seconds) happens at startup, not on first request

### Project Structure Alignment

[Source: architecture/unified-project-structure.md]

**Files to Create:**

- `backend/src/services/embedding.service.ts` - Core embedding logic
- `backend/src/queues/embedding.queue.ts` - Job queue implementation
- Update `backend/src/index.ts` - Add service initialization
- Update `backend/src/routes/health.routes.ts` - Add health status

**Testing Structure:**

- `backend/tests/unit/services/embedding.service.test.ts` - Service unit tests
- `backend/tests/unit/services/text-preprocessing.test.ts` - Text cleaning tests
- `backend/tests/integration/queues/embedding.queue.test.ts` - Queue integration tests
- `backend/tests/integration/routes/health.routes.test.ts` - Health endpoint tests

**Cache Directory:**

- `.cache/transformers/` - Auto-created by Transformers.js
- Should be in `.gitignore` (large models)
- Persisted across server restarts

### Error Handling Strategy

[Source: architecture/error-handling-strategy.md#Backend-Error-Handling]

**Error Types to Use:**

```typescript
import { AppError, NotFoundError } from '@/types/errors';

// When note not found during processing
throw new NotFoundError('Note');

// When service not initialized
throw new Error('Embedding service not initialized');
```

**Logging Approach:**

- Use `console.log()` / `console.error()` for now (Bun default)
- Format: `✅ Success message` / `❌ Error message`
- Include contextual info: noteId, retryCount, error message
- Log failures for debugging, but don't throw in queue processor

### Text Preprocessing Details

**Markdown Patterns to Remove:**

```typescript
// Symbols: # * _ ~ `
'# Title with *bold*' → 'Title with bold'

// Links: [text](url)
'See [docs](https://example.com)' → 'See docs'

// Multiple newlines → single space
'Line 1\n\nLine 2' → 'Line 1 Line 2'

// Whitespace trim
'  text  ' → 'text'

// Truncate to 2000 chars (≈512 tokens)
```

**Why Preprocess?**

- Embedding model trained on clean text, not raw markdown
- Link URLs add noise to semantic meaning
- Formatting symbols are not semantic content
- Truncation prevents context window overflow

**Testing Edge Cases:**

- Empty string
- Only markdown symbols
- Mixed languages (Latin + non-Latin)
- Special characters

### Testing Strategy

[Source: architecture/testing-strategy.md#Backend-Testing]

**Test Framework:** Bun Test (built-in)
**Test Pattern:** Jest-compatible API with Bun's built-in expect()

**Unit Test Patterns:**

```typescript
import { describe, test, expect, beforeEach } from 'bun:test';
import { embeddingService } from '@/services/embedding.service';

describe('EmbeddingService', () => {
  test('should initialize model on first use', async () => {
    await embeddingService.initialize();
    const status = embeddingService.getStatus();
    expect(status.isInitialized).toBe(true);
  });

  test('should generate 384-dimensional embedding', async () => {
    const embedding = await embeddingService.generateEmbedding('test');
    expect(embedding.length).toBe(384);
  });
});
```

**Integration Test Patterns:**

```typescript
import { describe, test, expect, beforeEach, afterEach } from 'bun:test';
import { db } from '@/db';
import { embeddingQueue } from '@/queues/embedding.queue';
import { notes, users } from '@/db/schema';

describe('EmbeddingQueue', () => {
  let testUserId: string;
  let testNoteId: string;

  beforeEach(async () => {
    // Create test user and note
    const [user] = await db
      .insert(users)
      .values({
        email: 'test@example.com',
        password_hash: 'hash',
        name: 'Test',
      })
      .returning();
    testUserId = user.id;

    const [note] = await db
      .insert(notes)
      .values({
        user_id: testUserId,
        title: 'Test',
        content: 'Test content',
      })
      .returning();
    testNoteId = note.id;
  });

  afterEach(async () => {
    // Clean up (cascade delete via user)
    await db.delete(users).where(eq(users.id, testUserId));
  });

  test('should enqueue and process embedding', async () => {
    await embeddingQueue.enqueue(testNoteId);

    // Wait for processing (poll with timeout)
    let completed = false;
    for (let i = 0; i < 30; i++) {
      const note = await db.query.notes.findFirst({
        where: eq(notes.id, testNoteId),
      });
      if (note.embedding_status === 'completed') {
        completed = true;
        break;
      }
      await new Promise((r) => setTimeout(r, 100));
    }

    expect(completed).toBe(true);
  });
});
```

### Performance Considerations

**Expected Performance:**

- Embedding generation: 300-500ms per note (CPU-bound)
- Queue processing: Up to 5 concurrent = 1500-2500ms for 5 notes
- Model loading: 2-3 seconds at startup (one-time cost)
- Memory overhead: ~120MB for cached model

**Optimization Notes for Future:**

- Current: In-memory queue (MVP)
- Future: Redis queue for multi-instance scaling
- Current: Sequential processing within MAX_CONCURRENT limit
- Future: Could use Worker Threads if CPU becomes bottleneck

### No External Dependencies

- No Redis queue needed (in-memory sufficient for MVP)
- No separate ML inference service (GPU/Docker container)
- No cloud embedding API (self-hosted, offline-first)
- All dependencies are npm packages installed in backend

---

## Testing

### Testing Standards

**Test File Locations:**

- Unit tests: `backend/tests/unit/services/embedding.service.test.ts`
- Unit tests: `backend/tests/unit/services/text-preprocessing.test.ts`
- Integration tests: `backend/tests/integration/queues/embedding.queue.test.ts`
- Integration tests: `backend/tests/integration/routes/health.routes.test.ts`

**Testing Frameworks:**

- Test Runner: Bun Test (built-in)
- Assertion: Bun's expect() API (Jest-compatible)
- Pattern: Jest-compatible describe/test/expect blocks

**Test Coverage Requirements:**

- Text preprocessing: All markdown patterns (symbols, links, newlines, truncation)
- Embedding generation: Dimension validation, error states, consistency
- Queue: Concurrent processing limits, duplicate prevention, retries, failures
- Health endpoint: Status structure and queue size tracking

**Specific Test Scenarios:**

1. Model initialization and caching
2. Embedding dimensionality (384)
3. Text cleaning (markdown removal)
4. Concurrent processing limit (MAX_CONCURRENT=5)
5. Duplicate processing prevention
6. Retry strategy (exponential backoff, max 3 attempts)
7. Permanent failure marking
8. Health endpoint data accuracy

---

## Change Log

| Date       | Version | Description                                               | Author       |
| ---------- | ------- | --------------------------------------------------------- | ------------ |
| 2026-02-17 | 0.1     | Initial story draft with embedding service implementation | Scrum Master |

---

## Dev Agent Record

_This section will be populated during development_

### Agent Model Used

[To be filled by dev agent]

### Debug Log References

[To be filled by dev agent]

### Completion Notes List

[To be filled by dev agent]

### File List

[To be filled by dev agent]

---

## QA Results

_This section will be populated after QA review_

[To be filled by QA agent]
